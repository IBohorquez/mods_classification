{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35268294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "655597ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tf/mnt/mario/projects/mods_v1/scripts/module_mods3d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tf/mnt/mario/projects/mods_v1/scripts/module_mods3d.py\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from random import randint, shuffle\n",
    "from skimage import io\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "import logging\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RepeatedStratifiedKFold\n",
    "# from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import optuna\n",
    "\n",
    "# from torchvision.models import vgg16, VGG16_Weights\n",
    "from torchvision.models import *\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "def binary_labeling_pos_neg(x):\n",
    "    if x.split('/')[7] == 'MODS_Positivas':\n",
    "        return 1\n",
    "    elif x.split('/')[7] == 'MODS_Negativas':\n",
    "        return 0\n",
    "\n",
    "def binary_labelling_7_9(x):\n",
    "    if x.split('/')[7] == 'MODS_Positivas':\n",
    "        if x.split('/')[-3].split('_')[1] == '7':\n",
    "            return 0\n",
    "        elif x.split('/')[-3].split('_')[1] == '9':\n",
    "            return 1\n",
    "    elif x.split('/')[7] == 'MODS_Negativas':\n",
    "        return np.nan\n",
    "    \n",
    "def multiclass_labeling_neg_7_9(x):\n",
    "    if x.split('/')[7] == 'MODS_Positivas':\n",
    "        if x.split('/')[-3].split('_')[1] == '7':\n",
    "            return 1\n",
    "        elif x.split('/')[-3].split('_')[1] == '9':\n",
    "            return 2\n",
    "    elif x.split('/')[7] == 'MODS_Negativas':\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_dataframe(path, mode='binary_pos_neg'):\n",
    "    \n",
    "    path_images = []\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            path_images.append(os.path.join(root, file))\n",
    "\n",
    "    df_data = pd.DataFrame({'path_images': path_images})\n",
    "    \n",
    "    df_data['img_name'] = df_data['path_images'].apply(lambda x: x.split('/')[-1])\n",
    "    \n",
    "    if mode == 'binary_pos_neg':\n",
    "        df_data['label'] = df_data['path_images'].apply(binary_labeling_pos_neg)\n",
    "    elif mode == 'multiclass_neg_7_9':\n",
    "        df_data['label'] = df_data['path_images'].apply(multiclass_labeling_neg_7_9)\n",
    "    elif mode == 'binary_7_9':\n",
    "        df_data['label'] = df_data['path_images'].apply(binary_labelling_7_9)\n",
    "        df_data.dropna(subset=['label'], inplace=True)\n",
    "        df_data.reset_index(drop=True, inplace=True)\n",
    "        df_data['label'] = df_data['label'].astype('int32')\n",
    "    \n",
    "    return df_data\n",
    "\n",
    "def split_dataframe(df_data, test_size=0.2, random_state=None):\n",
    "    \n",
    "    df_cross_val, df_test = train_test_split(df_data, \n",
    "                                             test_size=test_size, \n",
    "                                             random_state=random_state, \n",
    "                                             stratify=df_data['label'])\n",
    "\n",
    "    df_cross_val.reset_index(drop=True, inplace=True)\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df_cross_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7acf4c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /tf/mnt/mario/projects/mods_v1/scripts/module_mods3d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a /tf/mnt/mario/projects/mods_v1/scripts/module_mods3d.py\n",
    "\n",
    "\n",
    "class Dataset_MODS(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df_data = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_img = self.df_data['path_images'][idx]\n",
    "        label = self.df_data['label'][idx]\n",
    "        img = io.imread(path_img)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img.copy())\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7825712c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /tf/mnt/mario/projects/mods_v1/scripts/module_mods3d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a /tf/mnt/mario/projects/mods_v1/scripts/module_mods3d.py\n",
    "\n",
    "def moveTo(obj, device):\n",
    "\n",
    "    if isinstance(obj, list):\n",
    "        return [moveTo(x, device) for x in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return tuple(moveTo(list(obj), device))\n",
    "    elif isinstance(obj, set):\n",
    "        return set(moveTo(list(obj), device))\n",
    "    elif isinstance(obj, dict):\n",
    "        to_ret = dict()\n",
    "        for key, value in obj.items():\n",
    "            to_ret[moveTo(key, device)] = moveTo(value, device)\n",
    "        return to_ret\n",
    "    elif hasattr(obj, \"to\"):\n",
    "        return obj.to(device)\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d9411b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /tf/mnt/mario/projects/mods_v1/scripts/module_mods3d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a /tf/mnt/mario/projects/mods_v1/scripts/module_mods3d.py\n",
    "\n",
    "def run_epoch(model, optimizer, data_loader, loss_func, device, results, score_funcs, prefix=\"\", desc=None):\n",
    "    \n",
    "    running_loss = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for inputs, labels in tqdm(data_loader, desc=desc, leave=False):\n",
    "        \n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        \n",
    "        inputs = moveTo(inputs, device)\n",
    "        labels = moveTo(labels, device)\n",
    "\n",
    "        y_hat = model(inputs)\n",
    "        loss = loss_func(y_hat, labels)\n",
    "\n",
    "        if model.training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        running_loss.append(loss.item())\n",
    "\n",
    "        if len(score_funcs) > 0 and isinstance(labels, torch.Tensor):\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "            y_hat = y_hat.detach().cpu().numpy()\n",
    "            y_true.extend(labels.tolist())\n",
    "            y_pred.extend(y_hat.tolist())\n",
    "            \n",
    "    end = time.time()\n",
    "    \n",
    "    y_pred = np.asarray(y_pred)\n",
    "    \n",
    "    if len(y_pred.shape) == 2 and y_pred.shape[1] > 1: \n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    results[prefix + \" loss\"].append(np.mean(running_loss))\n",
    "\n",
    "    for name, score_func in score_funcs.items():\n",
    "        try:\n",
    "            if name == 'Accuracy':\n",
    "                results[prefix + \" \" + name].append(score_func(y_true, y_pred))\n",
    "            elif name == 'Precision':\n",
    "                results[prefix + \" \" + name].append(score_func(y_true, y_pred, pos_label=1))\n",
    "            elif name == 'Sensitivity':\n",
    "                results[prefix + \" \" + name].append(score_func(y_true, y_pred, pos_label=1))\n",
    "            elif name == 'Specificity':\n",
    "                results[prefix + \" \" + name].append(score_func(y_true, y_pred, pos_label=0))\n",
    "            elif name == 'F1_score':\n",
    "                results[prefix + \" \" + name].append(score_func(y_true, y_pred, pos_label=1))\n",
    "            elif name == 'ROC AUC Score':\n",
    "                results[prefix + \" \" + name].append(score_func(y_true, y_pred))\n",
    "            elif name == 'Quadratic Weighted Kappa':\n",
    "                results[prefix + \" \" + name].append(score_func(y_true, y_pred, weights='quadratic')) \n",
    "        except:\n",
    "            results[prefix + \" \" + name].append(float(\"NaN\"))\n",
    "            \n",
    "    return end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6deceb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /tf/mnt/mario/projects/mods_v1/scripts/module_mods3d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a /tf/mnt/mario/projects/mods_v1/scripts/module_mods3d.py\n",
    "\n",
    "def train_model(model, loss_func, train_loader, val_loader=None, test_loader=None, score_funcs=None, \n",
    "                epochs=10, device=\"cpu\", checkpoint_file=None, lr_schedule=None, optimizer=None, disable_tqdm=False):\n",
    "    \n",
    "    if score_funcs == None:\n",
    "        score_funcs = {}\n",
    "    \n",
    "    to_track = [\"epoch\", \"total time\", \"train loss\"]\n",
    "    \n",
    "    if val_loader is not None:\n",
    "        to_track.append(\"val loss\")\n",
    "    if test_loader is not None:\n",
    "        to_track.append(\"test loss\")\n",
    "        \n",
    "    for eval_score in score_funcs:\n",
    "        to_track.append(\"train \" + eval_score )\n",
    "        \n",
    "        if val_loader is not None:\n",
    "            to_track.append(\"val \" + eval_score )\n",
    "        if test_loader is not None:\n",
    "            to_track.append(\"test \"+ eval_score )\n",
    "    \n",
    "        \n",
    "    total_train_time = 0\n",
    "    results = {}\n",
    "    \n",
    "    for item in to_track:\n",
    "        results[item] = []\n",
    "\n",
    "    if optimizer == None:\n",
    "        optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Epoch\", disable=disable_tqdm):\n",
    "        \n",
    "        model = model.train()\n",
    "\n",
    "        total_train_time += run_epoch(model, optimizer, train_loader, loss_func, device, results, score_funcs, prefix=\"train\", desc=\"Training\")\n",
    "        \n",
    "        results[\"epoch\"].append(epoch)\n",
    "        results[\"total time\"].append(total_train_time)\n",
    "        \n",
    "      \n",
    "        if val_loader is not None:\n",
    "            model = model.eval()\n",
    "            with torch.no_grad():\n",
    "                run_epoch(model, optimizer, val_loader, loss_func, device, results, score_funcs, prefix=\"val\", desc=\"Validating\")\n",
    "                \n",
    "        if lr_schedule is not None:\n",
    "            if isinstance(lr_schedule, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                lr_schedule.step(results[\"val loss\"][-1])\n",
    "            else:\n",
    "                lr_schedule.step()\n",
    "                \n",
    "        if test_loader is not None:\n",
    "            model = model.eval()\n",
    "            with torch.no_grad():\n",
    "                run_epoch(model, optimizer, test_loader, loss_func, device, results, score_funcs, prefix=\"test\", desc=\"Testing\")\n",
    "        \n",
    "        if checkpoint_file is not None:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'results' : results\n",
    "                }, checkpoint_file)\n",
    "\n",
    "    return pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3212b436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /tf/mnt/mario/projects/mods_v1/scripts/module_mods3d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a /tf/mnt/mario/projects/mods_v1/scripts/module_mods3d.py\n",
    "\n",
    "def get_base_model(name_model='vgg16'):\n",
    "    \n",
    "    if name_model == 'vgg16':\n",
    "        weights = VGG16_Weights.DEFAULT\n",
    "        model = vgg16(weights=weights)\n",
    "        in_features = model.classifier[0].in_features\n",
    "    elif name_model == 'vgg16_bn':\n",
    "        weights = VGG16_BN_Weights.DEFAULT\n",
    "        model = vgg16_bn(weights=weights)\n",
    "        in_features = model.classifier[0].in_features\n",
    "        \n",
    "    return model, weights, in_features\n",
    "\n",
    "def get_optimizer(model, lr, name_optimizer):\n",
    "    \n",
    "    if name_optimizer == 'AdamW':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr)\n",
    "        \n",
    "    return optimizer\n",
    "\n",
    "def get_loss_func(name_loss_func):\n",
    "    \n",
    "    if name_loss_func == 'CrossEntropyLoss':\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    return loss_func\n",
    "\n",
    "def get_metric(name_metric='Accuracy'):\n",
    "    \n",
    "    if name_metric == 'Accuracy':\n",
    "        score_funcs = {name_metric: accuracy_score}\n",
    "    elif name_metric == 'Quadratic Weighted Kappa':\n",
    "        score_funcs = {name_metric: cohen_kappa_score}\n",
    "    \n",
    "    return score_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80eaef9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /tf/mnt/mario/projects/mods_v1/scripts/module_mods3d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a /tf/mnt/mario/projects/mods_v1/scripts/module_mods3d.py\n",
    "\n",
    "class NormalizeInput(nn.Module):\n",
    "    def __init__(self, baseModel, weights):\n",
    "        super(NormalizeInput, self).__init__()\n",
    "        self.baseModel = baseModel\n",
    "        self.preprocess = weights.transforms()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "        input = self.preprocess(input)\n",
    "        \n",
    "        return self.baseModel(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "377f1e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /tf/mnt/mario/projects/mods_v1/scripts/module_mods3d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a /tf/mnt/mario/projects/mods_v1/scripts/module_mods3d.py\n",
    "\n",
    "def get_hyperparameters(path, idx):\n",
    "    \n",
    "    df_hyper = pd.read_csv(os.path.join(path, 'hyperparameters.txt'), sep='\\t')\n",
    "    dict_hyper = {}\n",
    "    \n",
    "    ## Epochs\n",
    "    \n",
    "    df_n_epochs = pd.read_csv(os.path.join(path, 'n_epochs.txt'), sep='\\t')\n",
    "    \n",
    "    dict_hyper['n_epochs_min'] = int(df_n_epochs['n_epochs_min'][0])\n",
    "    dict_hyper['n_epochs_max'] = int(df_n_epochs['n_epochs_max'][0])\n",
    "    dict_hyper['n_epochs_step'] = int(df_n_epochs['n_epochs_step'][0])\n",
    "    dict_hyper['n_epochs_range'] = eval(df_n_epochs['n_epochs_range'][0])\n",
    "    \n",
    "    if np.isnan(df_hyper['n_epochs'][idx]):\n",
    "        dict_hyper['n_epochs'] = np.nan\n",
    "    else:\n",
    "        dict_hyper['n_epochs'] = int(df_hyper['n_epochs'][idx])\n",
    "    \n",
    "    \n",
    "    ## Batch size\n",
    "    \n",
    "    df_batch_size = pd.read_csv(os.path.join(path, 'batch_size.txt'), sep='\\t')\n",
    "\n",
    "    dict_hyper['batch_size_min'] = int(df_batch_size['batch_size_min'][0])\n",
    "    dict_hyper['batch_size_max'] = int(df_batch_size['batch_size_max'][0])\n",
    "    dict_hyper['batch_size_step'] = int(df_batch_size['batch_size_step'][0])\n",
    "    dict_hyper['batch_size_range'] = eval(df_batch_size['batch_size_range'][0])\n",
    "    \n",
    "    if np.isnan(df_hyper['batch_size'][idx]):\n",
    "        dict_hyper['batch_size'] = np.nan\n",
    "    else:\n",
    "        dict_hyper['batch_size'] = int(df_hyper['batch_size'][idx])\n",
    "    \n",
    "    \n",
    "    ## Hidden layers\n",
    "    \n",
    "    df_hidden_layers = pd.read_csv(os.path.join(path, 'hidden_layers.txt'), sep='\\t')\n",
    "\n",
    "    dict_hyper['hidden_layers_min'] = int(df_hidden_layers['hidden_layers_min'][0])\n",
    "    dict_hyper['hidden_layers_max'] = int(df_hidden_layers['hidden_layers_max'][0])\n",
    "    dict_hyper['hidden_layers_step'] = int(df_hidden_layers['hidden_layers_step'][0])\n",
    "    dict_hyper['hidden_layers_range'] = eval(df_hidden_layers['hidden_layers_range'][0])\n",
    "\n",
    "    if np.isnan(df_hyper['hidden_layers'][idx]):\n",
    "        dict_hyper['hidden_layers'] = np.nan\n",
    "    else:\n",
    "        dict_hyper['hidden_layers'] = int(df_hyper['hidden_layers'][idx])\n",
    "    \n",
    "    \n",
    "    ## Neurons per layer\n",
    "    \n",
    "    df_neurons_per_layer = pd.read_csv(os.path.join(path, 'neurons_per_layer.txt'), sep='\\t')\n",
    "    \n",
    "    dict_hyper['neurons_per_layer_min'] = int(df_neurons_per_layer['neurons_per_layer_min'][0])\n",
    "    dict_hyper['neurons_per_layer_max'] = int(df_neurons_per_layer['neurons_per_layer_max'][0])\n",
    "    dict_hyper['neurons_per_layer_step'] = int(df_neurons_per_layer['neurons_per_layer_step'][0])\n",
    "    dict_hyper['neurons_per_layer_range'] = eval(df_neurons_per_layer['neurons_per_layer_range'][0])\n",
    "\n",
    "    if np.isnan(df_hyper['neurons_per_layer'][idx]):\n",
    "        dict_hyper['neurons_per_layer'] = np.nan\n",
    "    else:\n",
    "        dict_hyper['neurons_per_layer'] = int(df_hyper['neurons_per_layer'][idx])\n",
    "    \n",
    "    \n",
    "    ## Learning rate\n",
    "    \n",
    "    df_learning_rate = pd.read_csv(os.path.join(path, 'learning_rate.txt'), sep='\\t')\n",
    "    \n",
    "    dict_hyper['learning_rate_min'] = float(df_learning_rate['learning_rate_min'][0])\n",
    "    dict_hyper['learning_rate_max'] = float(df_learning_rate['learning_rate_max'][0])\n",
    "    dict_hyper['learning_rate_range'] = eval(df_learning_rate['learning_rate_range'][0])\n",
    "    \n",
    "    if np.isnan(df_hyper['learning_rate'][idx]):\n",
    "        dict_hyper['learning_rate'] = np.nan\n",
    "    else:\n",
    "        dict_hyper['learning_rate'] = float(df_hyper['learning_rate'][idx])\n",
    "    \n",
    "    \n",
    "    ## Other Hyperparameters\n",
    "    \n",
    "    dict_hyper['for_optimization'] = str(df_hyper['for_optimization'][idx])\n",
    "    \n",
    "    if np.isnan(df_hyper['test_size'][idx]):\n",
    "        dict_hyper['test_size'] = 0.2\n",
    "    else:\n",
    "        dict_hyper['test_size'] = float(df_hyper['test_size'][idx])\n",
    "        \n",
    "    if type(df_hyper['random_state'][idx]) == str:\n",
    "        dict_hyper['random_state'] = None\n",
    "    else:\n",
    "        dict_hyper['random_state'] = int(df_hyper['random_state'][idx])    \n",
    "        \n",
    "    if np.isnan(df_hyper['val_size'][idx]):\n",
    "        dict_hyper['val_size'] = 0.25\n",
    "    else:\n",
    "        dict_hyper['val_size'] = float(df_hyper['val_size'][idx])\n",
    "    \n",
    "    if type(df_hyper['mode'][idx]) == str:\n",
    "        dict_hyper['mode'] = str(df_hyper['mode'][idx])\n",
    "    else:\n",
    "        dict_hyper['mode'] = 'binary_pos_neg'\n",
    "    \n",
    "    if type(df_hyper['base_model'][idx]) == str:\n",
    "        dict_hyper['base_model'] = str(df_hyper['base_model'][idx])\n",
    "    else:\n",
    "        dict_hyper['base_model'] = 'vgg16'\n",
    "    \n",
    "    if type(df_hyper['optimizer'][idx]) == str:\n",
    "        dict_hyper['optimizer'] = str(df_hyper['optimizer'][idx])\n",
    "    else:\n",
    "        dict_hyper['optimizer'] = 'AdamW'\n",
    "        \n",
    "    if type(df_hyper['loss_func'][idx]) == str:\n",
    "        dict_hyper['loss_func'] = str(df_hyper['loss_func'][idx])\n",
    "    else:\n",
    "        dict_hyper['loss_func'] = 'CrossEntropyLoss'\n",
    "        \n",
    "    if type(df_hyper['metric'][idx]) == str:\n",
    "        dict_hyper['metric'] = str(df_hyper['metric'][idx])\n",
    "    else:\n",
    "        dict_hyper['metric'] = 'Accuracy'\n",
    "        \n",
    "    if type(df_hyper['dataset'][idx]) == str:\n",
    "        dict_hyper['dataset'] = str(df_hyper['dataset'][idx])\n",
    "    else:\n",
    "        dict_hyper['dataset'] = 'val'\n",
    "        \n",
    "    if type(df_hyper['direction'][idx]) == str:\n",
    "        dict_hyper['direction'] = str(df_hyper['direction'][idx])\n",
    "    else:\n",
    "        dict_hyper['direction'] = 'maximize'\n",
    "        \n",
    "    if np.isnan(df_hyper['n_trials'][idx]):\n",
    "        dict_hyper['n_trials'] = np.nan\n",
    "    else:\n",
    "        dict_hyper['n_trials'] = int(df_hyper['n_trials'][idx])\n",
    "        \n",
    "    if np.isnan(df_hyper['n_jobs'][idx]):\n",
    "        dict_hyper['n_jobs'] = np.nan\n",
    "    else:\n",
    "        dict_hyper['n_jobs'] = int(df_hyper['n_jobs'][idx])\n",
    "        \n",
    "    return df_hyper, dict_hyper\n",
    "\n",
    "def get_time(start_time, end_time):\n",
    "    \n",
    "    total_time = end_time-start_time\n",
    "    \n",
    "    if total_time <= 60:\n",
    "        return str(total_time) + ' ' + 'sec'\n",
    "    elif 60 < total_time <= 3600:\n",
    "        return str(round(total_time/60, 2)) + ' ' + 'min'\n",
    "    elif 3600 < total_time <= 86400:\n",
    "        return str(round(total_time/3600, 2)) + ' ' + 'hours'\n",
    "    elif total_time > 86400:\n",
    "        return str(round(total_time/86400, 2)) + ' ' + 'days'\n",
    "    \n",
    "def save_logfile(path, total_results, test_results, used_hyperparams, other_hyperparams, idx, id_trial):\n",
    "    \n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format = '%(levelname)s - %(message)s', \n",
    "                        filename=os.path.join(path, 'logfiles_crossval_train_test','logfile_crossval_train_test_' + str(idx) + '_' + str(id_trial) + '.log'))\n",
    "    \n",
    "    logging.info('########################################################################################')\n",
    "    logging.info('CROSS-VALIDATION')\n",
    "    logging.info('HYPERPARAMETERS - batch_size: %d - n_epochs: %d - hidden_layers: %d - neurons_per_layer: %d - learning_rate: %f - n_folds: %d - n_iters: %d' % used_hyperparams)\n",
    "    logging.info('OTHER HYPERPARAMETERS - mode: %s - base_model: %s - optimizer: %s - loss_func: %s - test_size: %f' % other_hyperparams)\n",
    "    \n",
    "    if other_hyperparams[0].split('_')[0] == 'binary':\n",
    "        if other_hyperparams[0].split('_')[1] == 'pos':\n",
    "            logging.info('Class 0: Negative')\n",
    "            logging.info('Class 1: Positive')\n",
    "        elif other_hyperparams[0].split('_')[1] == '7':\n",
    "            logging.info('Class 0: 7 days')\n",
    "            logging.info('Class 1: 9 days') \n",
    "    elif other_hyperparams[0].split('_')[0] == 'multiclass':\n",
    "        logging.info('Class 0: Negative')\n",
    "        logging.info('Class 1: 7 days')\n",
    "        logging.info('Class 2: 9 days')\n",
    "        \n",
    "    \n",
    "    for i in range(total_results.shape[0]):\n",
    "        string_row = []\n",
    "        for name_col in total_results.columns:\n",
    "            string_row.append(str(name_col) + ': ' + str(total_results[name_col][i]))\n",
    "\n",
    "        logging.info(' - '.join(string_row))\n",
    "            \n",
    "    logging.info('########################################################################################')\n",
    "    logging.info('TRAINING-TESTING')\n",
    "    logging.info('HYPERPARAMETERS - batch_size: %d - n_epochs: %d - hidden_layers: %d - neurons_per_layer: %d - learning_rate: %f - n_folds: %d - n_iters: %d' % used_hyperparams)\n",
    "    logging.info('OTHER HYPERPARAMETERS - mode: %s - base_model: %s - optimizer: %s - loss_func: %s - test_size: %f' % other_hyperparams)\n",
    "    \n",
    "    if other_hyperparams[0].split('_')[0] == 'binary':\n",
    "        if other_hyperparams[0].split('_')[1] == 'pos':\n",
    "            logging.info('Class 0: Negative')\n",
    "            logging.info('Class 1: Positive')\n",
    "        elif other_hyperparams[0].split('_')[1] == '7':\n",
    "            logging.info('Class 0: 7 days')\n",
    "            logging.info('Class 1: 9 days') \n",
    "    elif other_hyperparams[0].split('_')[0] == 'multiclass':\n",
    "        logging.info('Class 0: Negative')\n",
    "        logging.info('Class 1: 7 days')\n",
    "        logging.info('Class 2: 9 days')\n",
    "    \n",
    "    for i in range(test_results.shape[0]):\n",
    "        string_row = []\n",
    "        for name_col in test_results.columns:\n",
    "            string_row.append(str(name_col) + ': ' + str(test_results[name_col][i]))\n",
    "\n",
    "        logging.info(' - '.join(string_row))\n",
    "\n",
    "    logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e827560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tf/mnt/mario/projects/mods_v1/scripts/hyper_optm_mods3d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tf/mnt/mario/projects/mods_v1/scripts/hyper_optm_mods3d.py\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "from module_mods3d import *\n",
    "\n",
    "# path_project = '/tf/mnt/mario/projects/mods_v1'\n",
    "# path_dataset = '/tf/mnt/mario/projects/mods_v1/dataset'\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Load index and path for hyperparameter optimization')\n",
    "parser.add_argument('-idx', type=int, help='Index of the set of hyperparameters')\n",
    "parser.add_argument('-path', type=str, help='Path of the project')\n",
    "args = parser.parse_args()\n",
    "\n",
    "idx = args.idx\n",
    "path_project = args.path\n",
    "\n",
    "\n",
    "df_hyper, dict_hyper = get_hyperparameters(os.path.join(path_project, 'optim_hyperparams'), idx=idx)\n",
    "\n",
    "\n",
    "\n",
    "## Epochs info\n",
    "\n",
    "n_epochs_min = dict_hyper['n_epochs_min']\n",
    "n_epochs_max = dict_hyper['n_epochs_max']\n",
    "n_epochs_step = dict_hyper['n_epochs_step']\n",
    "n_epochs_range = dict_hyper['n_epochs_range']\n",
    "\n",
    "\n",
    "## Batch size info\n",
    "\n",
    "batch_size_min = dict_hyper['batch_size_min']\n",
    "batch_size_max = dict_hyper['batch_size_max']\n",
    "batch_size_step = dict_hyper['batch_size_step']\n",
    "batch_size_range = dict_hyper['batch_size_range']\n",
    "\n",
    "\n",
    "## Hidden layers info\n",
    "\n",
    "hidden_layers_min = dict_hyper['hidden_layers_min']\n",
    "hidden_layers_max = dict_hyper['hidden_layers_max']\n",
    "hidden_layers_step = dict_hyper['hidden_layers_step']\n",
    "hidden_layers_range = dict_hyper['hidden_layers_range']\n",
    "\n",
    "\n",
    "## Neurons per layer info\n",
    "\n",
    "neurons_per_layer_min = dict_hyper['neurons_per_layer_min']\n",
    "neurons_per_layer_max = dict_hyper['neurons_per_layer_max']\n",
    "neurons_per_layer_step = dict_hyper['neurons_per_layer_step']\n",
    "neurons_per_layer_range = dict_hyper['neurons_per_layer_range']\n",
    "\n",
    "\n",
    "## Learning rate info\n",
    "\n",
    "learning_rate_min = dict_hyper['learning_rate_min']\n",
    "learning_rate_max = dict_hyper['learning_rate_max']\n",
    "learning_rate_range = dict_hyper['learning_rate_range']\n",
    "\n",
    "\n",
    "## Other hyperparameters\n",
    "\n",
    "hyper_params_for_optim = dict_hyper['for_optimization']\n",
    "mode = dict_hyper['mode']\n",
    "\n",
    "test_size = dict_hyper['test_size']\n",
    "val_size = dict_hyper['val_size']\n",
    "random_state = dict_hyper['random_state']\n",
    "\n",
    "direction = dict_hyper['direction']\n",
    "n_trials = dict_hyper['n_trials']\n",
    "n_jobs = dict_hyper['n_jobs']\n",
    "\n",
    "name_base_model = dict_hyper['base_model']\n",
    "name_optimizer = dict_hyper['optimizer']\n",
    "name_loss_func = dict_hyper['loss_func']\n",
    "name_metric = dict_hyper['metric']\n",
    "name_dataset = dict_hyper['dataset']\n",
    "\n",
    "n_epochs = dict_hyper['n_epochs']\n",
    "batch_size = dict_hyper['batch_size']\n",
    "hidden_layers = dict_hyper['hidden_layers']\n",
    "neurons_per_layer = dict_hyper['neurons_per_layer']\n",
    "learning_rate = dict_hyper['learning_rate']\n",
    "                       \n",
    "score_funcs = get_metric(name_metric=name_metric)\n",
    "\n",
    "\n",
    "## Getting dataset\n",
    "\n",
    "df_data = get_dataframe(os.path.join(path_project, 'dataset'), mode=mode)\n",
    "df_cross_val, df_test = split_dataframe(df_data, test_size=test_size, random_state=random_state)\n",
    "\n",
    "cross_val_dataset = Dataset_MODS(df_cross_val, transform=transforms.ToTensor())\n",
    "test_dataset = Dataset_MODS(df_test, transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "## Setting up device\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "## Optimization objective function\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    train_idx, val_idx = train_test_split(np.arange(len(cross_val_dataset)),\n",
    "                                          test_size=val_size,\n",
    "                                          shuffle=True,\n",
    "                                          stratify=cross_val_dataset.df_data.label)\n",
    "    \n",
    "    train_dataset = Subset(cross_val_dataset, train_idx)\n",
    "    val_dataset = Subset(cross_val_dataset, val_idx)\n",
    "    \n",
    "    if np.isnan(batch_size):\n",
    "        # bs = trial.suggest_int(name='batch_size', low=8, high=32, step=8)\n",
    "        bs = trial.suggest_int(name='batch_size', \n",
    "                               low=batch_size_min, \n",
    "                               high=batch_size_max, \n",
    "                               step=batch_size_step)\n",
    "    else:\n",
    "        bs = batch_size\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False)\n",
    "    \n",
    "    # Hidden layer size\n",
    "    if np.isnan(neurons_per_layer):\n",
    "        # neu_per_lay = trial.suggest_int(name='neurons_per_layer', low=16, high=512, step=16)\n",
    "        neu_per_lay = trial.suggest_int(name='neurons_per_layer', \n",
    "                                        low=neurons_per_layer_min, \n",
    "                                        high=neurons_per_layer_max, \n",
    "                                        step=neurons_per_layer_step)\n",
    "    else:\n",
    "        neu_per_lay = neurons_per_layer\n",
    "    \n",
    "    if np.isnan(hidden_layers):\n",
    "        # hidd_lay = trial.suggest_int(name='hidden_layers', low=1, high=4, step=1)\n",
    "        hidd_lay = trial.suggest_int(name='hidden_layers', \n",
    "                                     low=hidden_layers_min, \n",
    "                                     high=hidden_layers_max, \n",
    "                                     step=hidden_layers_step)\n",
    "    else:\n",
    "        hidd_lay = hidden_layers\n",
    "    \n",
    "    model, weights, in_features = get_base_model(name_model=name_base_model)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    classes = df_data['label'].unique().shape[0]\n",
    "\n",
    "    classifier_layers = [nn.Flatten(),\n",
    "                         nn.Linear(in_features, neu_per_lay),\n",
    "                         nn.ReLU()\n",
    "                        ]\n",
    "\n",
    "    for _ in range(hidd_lay-1):\n",
    "        classifier_layers.append(nn.Linear(neu_per_lay, neu_per_lay))\n",
    "        classifier_layers.append(nn.ReLU())\n",
    "    \n",
    "    classifier_layers.append(nn.Linear(neu_per_lay, classes))\n",
    "     \n",
    "    fc_classifier_model = nn.Sequential(*classifier_layers)\n",
    "    \n",
    "    model.classifier = fc_classifier_model\n",
    "    \n",
    "    model = NormalizeInput(model, weights)\n",
    "    \n",
    "    if np.isnan(learning_rate):\n",
    "        # lr = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "        lr = trial.suggest_float(name='learning_rate', \n",
    "                                 low=learning_rate_min, \n",
    "                                 high=learning_rate_max, \n",
    "                                 log=True)\n",
    "    else:\n",
    "        lr = learning_rate\n",
    "\n",
    "    optimizer = get_optimizer(model=model, lr=lr, name_optimizer=name_optimizer)\n",
    "    \n",
    "    loss_func = get_loss_func(name_loss_func=name_loss_func)\n",
    "    \n",
    "    if np.isnan(n_epochs):\n",
    "        # n_epoc = trial.suggest_int(name='n_epochs', low=10, high=100, step=10)\n",
    "        n_epoc = trial.suggest_int(name='n_epochs', \n",
    "                                   low=n_epochs_min, \n",
    "                                   high=n_epochs_max, \n",
    "                                   step=n_epochs_step)\n",
    "\n",
    "    else:\n",
    "        n_epoc = n_epochs\n",
    "        \n",
    "    results = train_model(model, loss_func, train_loader, val_loader=val_loader, score_funcs=score_funcs, \n",
    "                          epochs=n_epoc, device=device, optimizer=optimizer, disable_tqdm=True)\n",
    "    \n",
    "    return results[name_dataset + ' ' + name_metric].iloc[-1]\n",
    "\n",
    "\n",
    "## Optimization procedure\n",
    "\n",
    "def optimization():\n",
    "                          \n",
    "    search_space = {}\n",
    "                          \n",
    "    if np.isnan(batch_size):\n",
    "        # search_space['batch_size'] = [8, 16, 32]\n",
    "        search_space['batch_size'] = batch_size_range\n",
    "    if np.isnan(neurons_per_layer):\n",
    "        # search_space['neurons_per_layer'] = [16, 32, 64, 128, 256, 512]\n",
    "        search_space['neurons_per_layer'] = neurons_per_layer_range\n",
    "    if np.isnan(hidden_layers):\n",
    "        # search_space['hidden_layers'] = [1, 2, 3, 4]\n",
    "        search_space['hidden_layers'] = hidden_layers_range\n",
    "    if np.isnan(n_epochs):\n",
    "        # search_space['n_epochs'] = [10, 30, 50, 80, 100]\n",
    "        search_space['n_epochs'] = n_epochs_range\n",
    "    if np.isnan(learning_rate):\n",
    "        # search_space['learning_rate'] = [1e-5, 1e-4, 1e-3, 1e-2]\n",
    "        search_space['learning_rate'] = learning_rate_range\n",
    "    \n",
    "    \n",
    "#     search_space = {'batch_size': [8, 16, 32, 64], \n",
    "#                     'neurons_per_layer': [16, 32, 64, 128, 256, 512], \n",
    "#                     'hidden_layers': [1, 2, 3, 4], \n",
    "#                     'n_epochs': [10, 30, 50, 80, 100],\n",
    "#                     'learning_rate': [1e-5, 1e-4, 1e-3, 1e-2]}\n",
    "              \n",
    "    ### To save logging info of the optmization by trial\n",
    "    # filename = os.path.join(path_project, 'log_files_optimization', 'logfile_optim_' + str(idx) + '.log')\n",
    "    # optuna.logging.get_logger('optuna').addHandler(logging.FileHandler(filename=filename, mode='w'))\n",
    "    \n",
    "    sampler = optuna.samplers.GridSampler(search_space)\n",
    "    study_name = 'HYPERPARAMETER OPTIMIZATION WITH OPTUNA: ' + \\\n",
    "                  mode + ' - ' + name_metric + ' - ' + hyper_params_for_optim + ' - ' + str(search_space)\n",
    "    \n",
    "    study = optuna.create_study(study_name=study_name, \n",
    "                                direction=direction, \n",
    "                                sampler=sampler)\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=n_jobs)\n",
    "    \n",
    "    return study\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print()\n",
    "    print('START OPTIMIZATION - Optimization of:', hyper_params_for_optim)\n",
    "    print()\n",
    "    \n",
    "    study = optimization()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print()\n",
    "    print('END OPTIMIZATION')\n",
    "    print()\n",
    "    \n",
    "    total_time = get_time(start_time, end_time)\n",
    "    \n",
    "    \n",
    "    if np.isnan(n_epochs):\n",
    "        n_epochs = int(study.best_params['n_epochs'])\n",
    "        df_hyper.loc[idx, 'n_epochs'] = str(n_epochs)\n",
    "    if np.isnan(batch_size):\n",
    "        batch_size = int(study.best_params['batch_size'])\n",
    "        df_hyper.loc[idx, 'batch_size'] = str(batch_size)\n",
    "    if np.isnan(hidden_layers):\n",
    "        hidden_layers = int(study.best_params['hidden_layers'])\n",
    "        df_hyper.loc[idx, 'hidden_layers'] = str(hidden_layers)\n",
    "    if np.isnan(neurons_per_layer):\n",
    "        neurons_per_layer = int(study.best_params['neurons_per_layer'])\n",
    "        df_hyper.loc[idx, 'neurons_per_layer'] = str(neurons_per_layer)\n",
    "    if np.isnan(learning_rate):\n",
    "        learning_rate = float(study.best_params['learning_rate'])\n",
    "        df_hyper.loc[idx, 'learning_rate'] = str(learning_rate)\n",
    "\n",
    "    best_value = study.best_value\n",
    "    \n",
    "    df_hyper.loc[idx, 'best_value'] = best_value\n",
    "    df_hyper.loc[idx, 'time'] = str(total_time)\n",
    "    df_hyper.loc[idx, 'best_trial_number'] = str(study.best_trial.number)\n",
    "    \n",
    "    ### To save optimized hyperparameters\n",
    "    # df_hyper.to_csv(os.path.join(path_project, 'optim_hyperparams', 'hyperparameters.txt'), index=False, sep='\\t', mode='w+')\n",
    "    \n",
    "    print(study.best_params)\n",
    "    print('Best Value ' + '(' + name_dataset + ' ' + name_metric + '):', study.best_value)\n",
    "    print('Best trial number:', study.best_trial.number)\n",
    "    print('Total time:' + total_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6363e26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tf/mnt/mario/projects/mods_v1/scripts/crossval_train_mods3d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tf/mnt/mario/projects/mods_v1/scripts/crossval_train_mods3d.py\n",
    "\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "from module_mods3d import *\n",
    "\n",
    "# path_project = '/tf/mnt/mario/projects/mods_v1'\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Load info for cross validation training')\n",
    "\n",
    "parser.add_argument('-idx', type=int, help='Index of the set of hyperparameters')\n",
    "parser.add_argument('-path', type=str, help='Path of the project')\n",
    "parser.add_argument('-batch_size', type=int, help='Batch size')\n",
    "parser.add_argument('-n_epochs', type=int, help='Number of epochs')\n",
    "parser.add_argument('-hidden_layers', type=int, help='Number of hidden layers')\n",
    "parser.add_argument('-neurons_per_layer', type=int, help='Number of neurons per layer')\n",
    "parser.add_argument('-learning_rate', type=float, help='Learning rate value')\n",
    "parser.add_argument('-test_size_crossval_train', type=float, help='Test size for cross-validation and training experimentation')\n",
    "parser.add_argument('-n_folds', type=int, help='Number of k-fold cross validation')\n",
    "parser.add_argument('-n_iters', type=int, help='Number of iterations')\n",
    "parser.add_argument('-id_trial', type=int, help='Number of trial for different configurations in optimization')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "idx = args.idx\n",
    "path_project = args.path\n",
    "# run_id = args.run_id\n",
    "id_trial = args.id_trial\n",
    "\n",
    "\n",
    "df_hyper, dict_hyper = get_hyperparameters(os.path.join(path_project, 'optim_hyperparams'), idx=idx)\n",
    "\n",
    "\n",
    "## HYPERPARAMETERS\n",
    "\n",
    "batch_size = args.batch_size\n",
    "n_epochs = args.n_epochs\n",
    "hidden_layers = args.hidden_layers\n",
    "neurons_per_layer = args.neurons_per_layer\n",
    "learning_rate = args.learning_rate\n",
    "\n",
    "\n",
    "\n",
    "## OTHER HYPERPARAMETERS\n",
    "\n",
    "mode = dict_hyper['mode']\n",
    "# mode = 'binary_pos_neg'\n",
    "\n",
    "test_size = args.test_size_crossval_train\n",
    "# test_size = dict_hyper['test_size']\n",
    "# test_size = 0.2\n",
    "\n",
    "# val_size = dict_hyper['val_size']\n",
    "# val_size = 0.25\n",
    "\n",
    "\n",
    "random_state = dict_hyper['random_state']\n",
    "# random_state = None\n",
    "\n",
    "name_base_model = dict_hyper['base_model']\n",
    "name_optimizer = dict_hyper['optimizer']\n",
    "name_loss_func = dict_hyper['loss_func']\n",
    "name_metric = dict_hyper['metric']\n",
    "name_dataset = dict_hyper['dataset']\n",
    "\n",
    "## Getting dataset\n",
    "\n",
    "df_data = get_dataframe(os.path.join(path_project, 'dataset'), mode=mode)\n",
    "\n",
    "\n",
    "## Setting up device\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "n_folds = args.n_folds\n",
    "# n_folds = 4\n",
    "\n",
    "n_iters = args.n_iters\n",
    "# n_iters = 2\n",
    "\n",
    "\n",
    "## METRICS\n",
    "\n",
    "if mode.split('_')[0] == 'binary':\n",
    "    score_funcs = {'Accuracy': accuracy_score,\n",
    "                   'Precision': precision_score,\n",
    "                   'Sensitivity': recall_score,\n",
    "                   'Specificity': recall_score,\n",
    "                   'F1_score': f1_score,\n",
    "                   'ROC AUC Score': roc_auc_score,\n",
    "                   'Quadratic Weighted Kappa': cohen_kappa_score}\n",
    "    \n",
    "elif mode.split('_')[0] == 'multiclass':\n",
    "    score_funcs = {'Accuracy': accuracy_score}\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=None)\n",
    "\n",
    "    ## Defining Transfer learning model\n",
    "\n",
    "    model, weights, in_features = get_base_model(name_model=name_base_model)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    classes = df_data['label'].unique().shape[0]\n",
    "\n",
    "    classifier_layers = [nn.Flatten(),\n",
    "                         nn.Linear(in_features, neurons_per_layer),\n",
    "                         nn.ReLU()]\n",
    "\n",
    "    for _ in range(hidden_layers-1):\n",
    "        classifier_layers.append(nn.Linear(neurons_per_layer, neurons_per_layer))\n",
    "        classifier_layers.append(nn.ReLU())\n",
    "\n",
    "    classifier_layers.append(nn.Linear(neurons_per_layer, classes)) \n",
    "    fc_classifier_model = nn.Sequential(*classifier_layers)\n",
    "\n",
    "    model.classifier = fc_classifier_model\n",
    "    model = NormalizeInput(model, weights)\n",
    "\n",
    "    optimizer = get_optimizer(model=model, lr=learning_rate, name_optimizer=name_optimizer)\n",
    "    loss_func = get_loss_func(name_loss_func=name_loss_func)\n",
    "\n",
    "\n",
    "    ## Training K-fold Cross-Validation\n",
    "\n",
    "    cross_val_results = pd.DataFrame()\n",
    "    test_results = pd.DataFrame()\n",
    "\n",
    "    for i in range(n_iters):\n",
    "\n",
    "        df_cross_val, df_test = split_dataframe(df_data, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        cross_val_dataset = Dataset_MODS(df_cross_val, transform=transforms.ToTensor())\n",
    "        test_dataset = Dataset_MODS(df_test, transform=transforms.ToTensor())\n",
    "        \n",
    "        cross_val_loader = DataLoader(cross_val_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        print()\n",
    "        print('ITERATION:', i+1)\n",
    "\n",
    "        for k, (train_idxs, val_idxs) in enumerate(skf.split(X=np.arange(len(cross_val_dataset)), \n",
    "                                                             y=cross_val_dataset.df_data['label'])):\n",
    "            \n",
    "            print('Fold', k+1)\n",
    "\n",
    "            train_dataset = Subset(cross_val_dataset, train_idxs)\n",
    "            val_dataset = Subset(cross_val_dataset, val_idxs)\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            results = train_model(model, loss_func, train_loader, test_loader=test_loader, val_loader=val_loader, \n",
    "                                  score_funcs=score_funcs, epochs=n_epochs, device=device, optimizer=optimizer, \n",
    "                                  disable_tqdm=True)\n",
    "\n",
    "            results.insert(0, 'k_fold', n_epochs*[k+1])\n",
    "            results.insert(0, 'N_iteration', n_epochs*[i+1])\n",
    "    \n",
    "\n",
    "            if cross_val_results.empty:\n",
    "                cross_val_results = results.copy()\n",
    "            else:\n",
    "                cross_val_results = pd.concat([cross_val_results, results], ignore_index=True)\n",
    "\n",
    "            for layer in model.baseModel.classifier:\n",
    "                if hasattr(layer, 'reset_parameters'):\n",
    "                    layer.reset_parameters() \n",
    "        \n",
    "        print('Training and testing on iteration', i+1)\n",
    "        \n",
    "        results = train_model(model, loss_func, cross_val_loader, test_loader=test_loader, score_funcs=score_funcs, \n",
    "                              epochs=n_epochs, device=device, optimizer=optimizer, disable_tqdm=True)\n",
    "        \n",
    "        results.insert(0, 'N_iteration', n_epochs*[i+1])\n",
    "        \n",
    "        if test_results.empty:\n",
    "            test_results = results.copy()\n",
    "        else:\n",
    "            test_results = pd.concat([test_results, results], ignore_index=True)\n",
    "            \n",
    "        for layer in model.baseModel.classifier:\n",
    "                if hasattr(layer, 'reset_parameters'):\n",
    "                    layer.reset_parameters()\n",
    "            \n",
    "        print()\n",
    "        \n",
    "        \n",
    "    cross_val_results.to_csv(os.path.join(path_project, 'csvfiles_crossval','csvfile_crossval_' + str(idx) + '_' + str(id_trial) + '.csv'), index=False)\n",
    "    test_results.to_csv(os.path.join(path_project, 'csvfiles_test','csvfile_test_' + str(idx) + '_'+ str(id_trial) + '.csv'), index=False)\n",
    "    \n",
    "    used_hyperparams = (batch_size, n_epochs, hidden_layers, neurons_per_layer, learning_rate, n_folds, n_iters)\n",
    "    other_hyperparams = (mode, name_base_model, name_optimizer, name_loss_func, test_size)\n",
    "    save_logfile(path_project, cross_val_results, test_results, used_hyperparams, other_hyperparams, idx, id_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1483f47b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
